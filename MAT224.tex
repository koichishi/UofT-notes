\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by Yuchen Wang}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\litran}[0]{$T: V \rightarrow W$ }
\newcommand{\slitran}[0]{Let $ T: V \rightarrow W$ be a linear transformation }
\newcommand{\mt}[0]{$[T]_\alpha^\beta$ }
% Attr.
\title{MAT224 Linear Algebra II \\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Vector Spaces}
	\subsection{Bases And Dimension (Jan 17)}
	\paragraph{Definition} A subset S of vector space V is called a \ti{basis} of V if V = Span(S) and S is linearly independent.
	\paragraph{Examples} 
	\begin{enumerate}
		\item the standard basis S = \{\tb{$e_1$},...,\tb{$e_n$}\} in $\mb{R}^n$, since every vector $(a_1, ..., a_n) \in \mb{R}^n$ may be written as the linear combination $(a_1,..., a_n) = a_1e_1+ ... + a_ne_n$
		\item The vector space $\mb{R}^n$ has many other bases as well. e.g., in $\mb{R}^2$, consider the set $S = \{(1,2),(1,-1)\}$, which is l.i.
		\item Let $V = P_n(\mb{R})$ and consider $ S = \{1, x, x^2, ..., x^n\}$, which is a basis of V.
		\newline
		\proof It is clear that S spans V. For independence, consider
		$$a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n = \tb{0}$$
		Take the derivative of both sides,
		$$\frac{d^n}{dx^n}(a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n) = \frac{d^n}{dx^n}(0)$$
		$$n!a_n = 0 \implies a_n = 0$$
		Similarly, we have $a_i = 0$ for all $i$, as wanted.
		
		\item The empty subset, $\emptyset$, is a basis of the vector space consisting only of a zero vector, $\{\tb{0}\}$.
	\end{enumerate}
	\paragraph{Theorem 1.6.3} Let V be a vector space, and let S be a nonempty subset of V. Then S is a basis of V iff every vector $\tb{x} \in V$ may be written uniquely as a linear combination of the vectors in S. \newline
	\under{\ti{Proof:}}
	$\rightarrow:$ Assume  S is a basis of V, then given $\tb{x} \in V$, there are scalars $a_i \in \mb{R}$ and vectors $x_i \in S$ s.t. $\tb{x} = a_1x_1 + ... + a_nx_n$. To show this linear combination is unique, consider a possible second linear combination of vectors in S which also adds up to \tb{x}: $x= b_1x_1 + ...+b_nx_n$. Subtracting these two expressions for $\tb{x}$, we find that
	$$\tb{0} = a_1x_1 + ... + a_nx_n - (b_1x_1 + ...+b_nx_n)$$ 
	$$=(a_1 - b_1)x_1 + ...+(a_n-b_n)x_n$$
	\under{Since S is linearly independent}, the equation implies that $a_i = b_i$ for all $i$.\newline\newline
	$\leftarrow$: Assume every vector $\tb{x} \in V$ may be written uniquely as a linear combination of the vectors in S. This implies $Span(S) = V$. We must show that S is l.i. Consider an equation
	$$a_1x_1 +...+a_nx_n = \tb{0}$$
	Note that it is also the case that
	$$0\tb{x} = 0(x_1 + ...+x_n) = \tb{0}$$
	Since we assumed that every \tb{x} has a unique representation in S, then it must be true that $a_i = 0$ for all $i$. Hence S is l.i.
 	\paragraph{Theorem 1.6.6} Let V be a vector space that has a finite spanning set, and let S be a linearly independent subset of V. Then there exists a basis S' of V, with $S \subset S'$
	\paragraph{Lemma 1.6.8} Let S be a linearly independent subset of V and let $x \in V$, but $x \notin S$. Then $S \cup \{\tb{x}\}$ is l.i. iff $\tb{x} \notin Span(S)$.
	\paragraph{Insight} the number of vectors in a basis is, in a rough sense, a measure of "how big" the space is.
	\paragraph{Theorem 1.6.10 (Basis Theorem)} Let V be a vector space and let S be a spanning set for V, which has m elements. Then no linearly independent set in V can have more than m elements.\newline
	\ti{\under{proof:}} It suffices to show that every set in V with more than m elements is linearly dependent. Write $S = {y_1, ..., y_m}$ and suppose $S' = {x_1, ..., x_n}$ is a subset of V with $n >m$ vectors. Consider an equation
	$$(1)a_1x_1 + ... + a_nx_n = \tb{0}$$
	Our goal is to show that $a_i$ not all 0.
	Since S spans V, there are scalars $b_{ij}$ s.t. for each $i$,
	$$x_i = b_{i1}y_1 + ... +b_{im}y_m$$
	Substituting these equations into (1), we get $$a_1(b_{11}y_1+...+b_{1m}y_m)+...+a_n(b_{n1}y_1+...+b_{nm}y_m)=\tb{0}$$
	Collecting terms and rearranging,
	$$(a_1b_{11}+...+a_nb_{n1})y_1+...+(a_1b_{1m}+...+a_nb_{nm})y_m = \tb{0}$$
	Since S is l.i., this is equivalent to solving the system
	$$b_{11}a_1+...+b_{n1}a_n = 0$$
	$$.$$
	$$.$$
	$$.$$
	$$b_{1m}a_1+...+b_{nm}a_n=0$$
	But this is a system with n unknowns and m equations and $n>m$, so there must exist a non-trivial solution $\{a_1,...,a_n\}$, which is what we wanted to show. QED
	\paragraph{Corollary 1.6.11} Let V be a vector space and let S and S' be two bases of V, with m and m' elements, respectively. Then m = m'.\newline
	\proof\newline
	Since S is a spanning set of V and S' is l.i., we have $m'\leq m$. Since S' is a spanning set of V and S is l.i.m we have $m \leq m'$. Hence $m = m'$. QED
	\paragraph{Definitions 1.6.12}
	\begin{enumerate}
		\item If V is a vector space with some finite basis(possibly empty), we say V is \under{\it{finite-dimentional}}.
		\item Let V be a finite-dimensional vector space. The dimension of V, denoted dim(V), is the number of vectors in a (hence any) basis of V.
		\item If $V = \{\tb{0}\}$, we define dim(V) = 0.
		\item dim$(span\{(1,2,3),(4,5,6),(7,8,9)\}) = 2$
	\end{enumerate}
	\paragraph{Examples}
	\begin{enumerate}
		\item For each n, dim($\mb{R}^n$) = n, since the standard basis contains n vectors.
		\item dim($P_n(\mb{R})) = n + 1$, since a basis for $P_n(\mb{R})$ contains n + 1 functions.
		\item The vector spaces $P(\mb{R}), C^1(\mb{R})$ and $C(\mb{R})$ are not finite-dimensional. We say that such spaces are \under{\it{infinite-dimentional}}.
	\end{enumerate}
	\paragraph{Corollary 1.6.14} Let W be a subspace of a finite-dimensional vector space V. Then $dim(W) \leq dim(V).$ Furthermore, $	dim(W) = dim(V)$ iff $W = V$.
	
	\paragraph{Corollary 1.6.15} Let W be a subspace of $\mb{R}^n$ defined by a system of homogeneous linear equations. Then dim(W) is equal to the number of free variables in the corresponding echelon form system.
	\paragraph{Theorem 1.6.18} Let $W_1$ and $W_2$ be finite-dimensional subspaces of a vector space V. Then $$dim(W_1 + W_2) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$$
	\paragraph{Remark} Analogous to the Principle of Inclusion-Exclusion \newline
	\proof Result obvious if either $W_1$ or $W_2$ is \{\tb{0}\}. \newline
	Therefore, we assume that neither $W_1$ nor $W_2$ is  \{\tb{0}\}. Starting from a basis S of $W_1 \cap W_2$. We can always find sets $T_1$ and $T_2$ (disjoint from S) such that $S \cup T_1$ is a basis for $W_1$ and $S \cup T_2$ is a basis for $W_2$. We claim that $U = S \cup T_1 \cup T_2$ is a basis for $W_1 + W_2$, since 
	$$ U = S \cup T_1 \cup T_2 = (S \cup T_1) \cup (S\cup T_2)$$
	$$Span(U) = Span((S \cup T_1) \cup (S\cup T_2)) = W_1 + W_2$$
	It remains to prove that U is linearly independent. Any potential linear dependence among the vectors in U must have the form 
	$$\tb{v} + \tb{w}_1 + \tb{w}_2 = \tb{0}$$
	where $\tb{v} \in Span(S) = W_1 \cap W_2, \tb{w}_1 \in Span(T_1) \subset W_1, \tb{w}_2 \in Span(T_2) \subset W_2$. (slice the linear combination into the sum of the vectors from 3 vector spaces). It suffices to prove that in any such potential linear dependence, we must have $\tb{v} = \tb{w}_1 = \tb{w}_2 = \tb{0}$ (each vector is a lin comb, and equals \tb{0}).  \newline
	Consider $\tb{w}_2 = -\tb{v} - \tb{w}_1$. Since $-\tb{v} - \tb{w}_1 \in W_1, \tb{w}_2 \in W_2,$ we must have $\tb{w}_2 \in W_1 \cap W_2$. By definition, $\tb{w}_2 \in Span(T_2)$ But $S \cap T_2 = \emptyset$, hence $Span(S) \cap Span(T_2) = \{\tb{0}\}.$ Therefore we must have $\tb{w}_2 = \tb{0}$. So then $-\tb{v} = \tb{w}_1 \in W_1 \cap W_2$. Since $S \cap T_1 = \emptyset$, $Span(S) \cap Span(T_1) = \{\tb{0}\}$ and we have $\tb{w}_1 = \tb{0}$, so $\tb{v} = \tb{0}$ as well. QED
	\paragraph{Excercises for 1.4}
	1.(k), 7
	\paragraph{Exercises for 1.6}
	1.(d)(e)(f), 3, 4, 16
	\newpage
	\section{Linear Transformations}
	\subsection{Linear Tranformations}
	A function T from V to W is denoted by $T: V \rightarrow W$. The vector $\tb{w} = T(\tb{v})$ in W is called the image of \tb{v} under the function T. Loosely speaking, we want our functions to turn the algebraic operations of addition and scalar multiplication in V into addition and scalar multiplication in W.
	\paragraph{Definition 2.1.1} A function $T: V \rightarrow W$ is called a \ti{linear mapping} or a \ti{linear transformation} if it satisfies
	\begin{enumerate}
		\item $T(\tb{u} + \tb{v}) = T(\tb{u}) + T(\tb{v})$ for all \tb{u} and \tb{v} $\in$ V
		\item $T(a\tb{v}) = aT(\tb{v})$ for all $a \in \mb{R}$ and $\tb{v} \in V$
	\end{enumerate}
	V is called the \ti{domain} of T and W is called the \ti{target} of T. \newline
	We say that a linear transformation preserves the operations of addition and scalar multiplication.
	\paragraph{Property} A linear mapping always takes the zero vector in the domain vector space to the zero vector in the target vector space.
	\paragraph{Proposition 2.1.2} A function \litran is a linear transformation if and only if for all $a$ and $b \in \mb{R}$ and all $\tb{u}$ and $\tb{v} \in V$
	$$T(a\tb{u} + b\tb{v}) = aT(\tb{u}) + bT(\tb{v})$$
	\paragraph{Corollary 2.1.3} A function \litran is a linear transformation if and only if for all $a_1, .., a_k \in \mb{R}$ and for all $\tb{v}_1, ..., \tb{v}_k \in V$:
	$$T(\sum^k_{i = 1} a_i \tb{v}_i) = \sum^k_{i = 1}a_iT(\tb{v}_i)$$
	\paragraph{Examples}
	\begin{enumerate}
		\item Let V be any vector space, and let W = V. The \ti{under{identity transformation}} $I: V \rightarrow V$ is defined by I(\tb{v}) = \tb{v} for all \tb{v} $\in$ V.
		\item Let V and W be any vector spaces, and let \litran be the mapping that takes every vector in V to the zero vector in W:
		$$ T(\tb{v}) = \tb{0}_W$$
		for all \tb{v} $\in$ V. T is called \it{\under{zero transformation}}.
		\item $T(\tb{x}) = a_1x_1 + ... + a_nx_n$
		\item Differentiation, definite integration
	\end{enumerate} 
	\paragraph{Remark} The inner product plays a crucial role in linear algebra in that it provides a bridge between algebra and geometry, which is the heart of the more advanced material that appears later in the text.
	\paragraph{Proposition 2.1.14} If \litran is a linear transformation and V is finite-dimensional, then T is uniquely determined by its values on the members of a basis of V. \newline
	\proof Show that if S and T are linear transformations that take the same values on each member of a basis for V, then in fact S = T. \newline
	\begin{align*}
		T(v) &= T(a_1v_1 + ... + a_kv_k)\\
		&= a_1T(v_1) + ... + a_kT(v_k)\\
		&= a_1S(v_1) + ... + a_kS(v_k)\\
		&= S(a_1v_1 + ... + a_kv_k)\\
		&= S(v)
	\end{align*}
	Therefore, S and T are equal as mappings from V to W. $\blacksquare$
	\subsection{Linear Transformations Between Finite-Dimensional Vector Spaces}
	\paragraph{Proposition 2.2.1} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W. If $\{\tb{v}_1,...,\tb{v}_k\}$ is a basis for V and $\{\tb{w}_1,...,\tb{w}_l\}$ is a basis for W, then \litran is uniquely determined by the $l\cdot k$ scalars used to express $T(\tb{v}_j), j = 1,...,k$, in terms of $\tb{w}_1, ..., \tb{w}_l$.
	\paragraph{Definition 2.2.6} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W, and let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$, respectively, be any bases for V and W. Let $a_ij, 1 \leq i \leq l$ and $1 \leq j\leq k$ be the $l \cdot k$ scalars that determine T with respect to the bases $\alpha$ and $\beta$. The matrix whose entries are the scalars $a_ij$, $1 \leq i \leq l$ and $1 \leq j\leq k$, is called the {\ti{matrix of the linear transformation T with respect to the bases $\alpha$ for V and $\beta$ for W}. This matrix is denoted by $[T]^\beta_\alpha$.
	\paragraph{Remark} The basis vectors in the domain and target spaces are written in some particular order.
	\paragraph{Definition of coordinate vectors} If $\tb{v} = a_1\tb{v}_1+...+a_k\tb{v}_k$ and $\tb{w} = b_1\tb{w}_1+...+b_l\tb{w}_l$, we can express \tb{v} and \tb{w} in coordinates, respectively, as a $k \times 1$ matrix and as an $l \times 1$ matrix, with respect to the chosen bases. These coordinate vectors will be denoted by $[\tb{v}]_\alpha$ and $[\tb{w}]_\beta$, respectively. Thus
	$[\tb{v}]_\alpha = \begin{bmatrix} 
a_1 \\
\vdots\\
a_k
\end{bmatrix}$
and
	$[\tb{w}]_\beta = \begin{bmatrix} 
b_1 \\
\vdots\\
b_l
\end{bmatrix}$
\paragraph{Proposition 2.2.15} Let \litran be a linear transformation between vector spaces V of dimension k and W of dimension $l$. Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W. Then for each $\tb{v} \in V$, $$[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$$
\proof Let $\tb{v} = x_1\tb{v}_1 + ...+x_k\tb{v}_k \in V$. Then if $T(\tb{v}_j) = a_{1j}\tb{w}_1 + ...+ a_{lj}\tb{w}_l$
\begin{align*}
	T(\tb{v}) &= \sum_{j=1}^kx_jT(\tb{v}_j)\\
	&=\sum_{j=1}^kx_j(\sum_{i=1}^la_{ij}\tb{w}_i)\\
	&=\sum_{i=1}^l(\sum_{j=1}^kx_ja_{ij})\tb{w}_i
\end{align*}
Thus, the $i$th coefficient of T(\tb{v}) in terms of $\beta$ is $\sum_{j=1}^kx_ja_{ij}$ and 
$[T(\tb{v})]_\beta = \begin{bmatrix}
	\sum_{j=1}^kx_ja_{1j}\\
	\vdots\\
	\sum_{j=1}^kx_ja_{lj}
\end{bmatrix}$ which is precisely $[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$. $\blacksquare$

\paragraph{Proposition 2.2.19} 
 Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W, and let $\tb{v} = x_1\tb{v}_1 +...+x_k\tb{v}_k \in V$
 \begin{enumerate}
 	\item If A is an $l \times k$ matrix, then the function $$T(\tb{v}) = \tb{w}$$
 	where $[\tb{w}]_\beta = A[\tb{v}]_\alpha$ is a linear transformation.
 	\item If $A = [S]_\alpha^\beta$ is the matrix of a transformation $S: V \rightarrow W$, then the transformation T constructed from $[S]_\alpha^\beta$ is equal to S.
 	\item If T is the transformation of (1) constructed from A, then $[T]_\alpha^\beta = A$
 \end{enumerate}
 
 \paragraph{Proposition 2.2.20} Let V and W be finite-dimensional vector spaces. Let $\alpha$ be a basis for V and $\beta$ a basis for W. Then the assignment of a matrix to a linear transformation from V to W given by T goes to $[T]_\alpha^\beta$ is injective and surjective.
 \paragraph{Notes}
 \begin{enumerate}
 	\item When proving a function T is not a linear transformation, can consider $T(\tb{0}) \neq \tb{0}$.
 \end{enumerate}
 \subsection{Kernel and Image}
 \paragraph{Definition 2.3.1} The $kernel$ of T, denoted $Ker(T)$, is the subset of V consisting of all vectors $\tb{v} \in V$ such that $T(\tb{v}) = 0$.
 \paragraph{Proposition 2.3.2} Let \litran be a linear transformation. Ker(T) is a subspace of V.
 \paragraph{Examples}
 \begin{enumerate}
 	\item Let $V = P_3(\mb{R})$. Define $T: V \rightarrow V$ by $T(p(x)) = \frac{d}{dx}p(x)$. Ker(T) only consists constant polynomials.
 	\item Let $V = W = \mb{R}^2$. Let T be a rotation $R_\theta$. Then $Ker(T) = \{\tb{0}\}$.
 \end{enumerate}
 \paragraph{Proposition 2.3.7} \slitran of finite-dimensional vector spaces, and let $\alpha$ and $\beta$ be bases for V and W, respectively. Then $\tb{x} \in Ker(T)$ ife lf the coordinate vector of \tb{x}, $[\tb{x}]_\alpha$, satisfies the system of equations
 $$ a_{11}x_1+...+a_{1k}x_k = 0$$
 $$\vdots$$
 $$ a_{l1}x_1+...+a_{lk}x_k = 0$$
 where the coefficients $a_{ij}$ are the entries of the matrix \mt.
\paragraph{Proposition 2.3.8} Let V be a finite-dimensional vector space, and let $\alpha = \{\tb{v}_1, ..., \tb{v}_k \}$ be a basis for V. Then the vectors $\tb{x}_1,...,\tb{x}_m \in V$ are linearly independent iff there corresponding coordinate vectors $[\tb{x}_1]_\alpha, ..., [\tb{x}_m]_\alpha$ are linearly independent.

 
	
\end{document}

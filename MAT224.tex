\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by Yuchen Wang}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\litran}[0]{$T: V \rightarrow W$ }
\newcommand{\slitran}[0]{Let $ T: V \rightarrow W$ be a linear transformation }
\newcommand{\mt}[0]{$[T]_\alpha^\beta$ }
\newcommand{\qed}[0]{$\hfill\blacksquare$}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\vx}[0]{\tb{x}}
\newcommand{\vy}[0]{\tb{y}}
\newcommand{\vz}[0]{\tb{z}}
\newcommand{\vo}[0]{\tb{0}}

% Attr.
\title{MAT224 Linear Algebra II \\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Vector Spaces}
	\subsection{Vector Spaces}
	\paragraph{Definition 1.1.1} A (real) vector space is a set V (whose elements are called vectors) together with
	\begin{enumerate}
		\item an operation called vector addition, which for each pair of vectors $\vx, \vy \in V$ produced another vector in V denoted $\vx + \vy$, and
		\item an operation called multiplication by a scalar (a real number), which for each vector $\vx \in V$, and each scalar $c \in \real$ produced another vector in V denoted $c\vx$
	\end{enumerate}
	Furthermore, the two operations must satisfy the following axioms:
	$\forall \vx, \vy, \vz \in V, \forall c, d \in \real$,
	\begin{enumerate}
		\item $(\vx + \vy) + \vz = \vx + (\vy + \vz)$
		\item $\vx + \vy = \vy + \vx$
		\item $\exists \vo \in V$ s.t. $\vx + \vo = \vx$ (additive identity)
		\item $\exists -\vx \in V$ s.t. $\vx + -\vx = \vo$ (additive inverse)
		\item $c(\vx + \vy) = c\vx + c\vy$
		\item $(c + d)\vx = c\vx + d\vx$
		\item $(cd)\vx = c(d\vx)$
		\item $1\vx = \vx$
	\end{enumerate}
	\paragraph{Smooth functions}
	$C^\infty$ \newline
	Most functions are not smooth.
	\subsection{Subspaces}
	\paragraph{Example}
	$C^\infty(\real) < C^k(\real) <$ Differentiable functions $< C(\real) < F(\real)$  
	\paragraph{Definition} Let V be a vector space and Let $W \subseteq V$ be a subset. Then W is a (vector) subspace of V if W is a vector space itself under the operations of vector sum and scalar multiplication from V.
	\paragraph{Theorem 1.2.8}  Let V be a vector space and Let $W \subseteq V$ be a \textcolor{red}{nonempty} subset of V. Then W is a subspace of V iff $\forall \vx, \vy \in W$, and all $c \in \real$, we have $c\vx + \vy \in W$. \newline
	\begin{proof}
	$\rightarrow$: If W is a subspace of V, then $\forall \vx, \vy \in W$ and $c \in \real, c\vx + \vy \in W$ holds since W itself is a real vector space. \newline
	$\leftarrow$: If $\forall \vx, \vy \in W$, and all $c \in \real$, we have $c\vx + \vy \in W$ \newline
	Can have $c = 1$, so $\vx + \vy \in W$ (close under addition) \newline
	$c = -1$ and $\vy = \vx$, so $-\vx + \vx = \vo \in W$ (additive identity) \newline
	$\vy = \vo$, so  $c\vx \in W$ (close under scalar multiplication) \newline
	These implies all the axioms. \qed
	\end{proof}

	\paragraph{Examples}
	\begin{enumerate}
		\item $W = \{f\in C(\real) | f(\pi) = 0\}$. W subspace of $C(\real)$? -Yes
		\item $W = \{f\in C(\real) | f(e) = e\}$. W subspace of $C(\real)$? -No, not close under addition
		\item $W = \{(x_1,...,x_n) | x_i\geq 0 \forall i\}$. W subspace of $C(\real)$? -No, there is no additive inverse for each item in W.
	\end{enumerate}
	\paragraph{Theorem 1.2.13} Let V be a vector space. Then the intersection of any collection of subspaces of V is a subspace of V. \newline
	\begin{proof}
		Consider any collection of subspace of V. Note that the intersection of the subspaces is not empty since at least the zero vector from V is in it. Now let $\vx, \vy$ be any two vectors in the intersection, so they are in every single subspace in the collection. Therefore $c\vx + \vy$ is also in every single subspace in the collection, so that it is in the intersection as well. Hence the intersection is a subspace of V. \qed
	\end{proof}
	\paragraph{Application} The set of all solutions of any simultaneous system of equations is a subspace of $\real^n$.
	\paragraph{Corollary 1.2.14} Let $a_{ij} (1 \leq i \leq m, 1\leq j \leq n)$ be any real numbers and let $W = \{(x_1,...,x_n) \in \real^n | a_{i1}x_1 + \hdots +a_{in}x_n = 0 \mbox{ for all } i, 1 \leq i \leq m\}$. Then W is a subspace of $\real^n$.
	
	
	\subsection{Linear Combinations}
	\paragraph{Definition 1.3.1} Let S be a subset of a vector space V.
	\begin{enumerate}
		\item A \ti{linear combination} of vectors in S is any sum $a_1\vx_1 + \hdots +  a_n \vx_n$ where the $a_i \in \real$, and the $\vx_i \in S$
		\item If $S \neq \emptyset$, the set of all linear combinations of vectors in S is called the \ti{span} of S, and denoted Span(S). \textcolor{red}{If $S = \emptyset$, we define Span(S) = \{\vo\}}. (Remark: It is a mathematician convention)
		\item If W = Span(S), we say S spans (or generates) W.
	\end{enumerate}
	\paragraph{Theorem 1.3.4} Let V be a vector space and let S be any subset of V. Then Span(S) is a subspace of V. \newline\newline
	\begin{proof}
		Span(S) is non-empty by definition. Let $\vx, \vy \in Span(S)$, then they are linear combinations of vectors in S. Check that $c\vx + \vy$ is also a linear combination of vectors in S, so $c\vx + \vy \in Span(S)$. Hence Span(S) is a subspace of V. \qed
	\end{proof}
	\paragraph{Definition} Let $W_1$ and $W_2$ be subspaces of a vector space V. The \ti{sum} of $W_1$ and $W_2$ is the set $$W_1 + W_2 = \{ \vx \in V | \vx = \vx_1 + \vx_2, \mbox{ for some } \vx_1 \in W_1 \mbox{ and } \vx_2 \in W_2\}$$
	\paragraph{Proposition 1.3.8 The basis of sum is the union of two bases} Let $W_1 = Span(S_1)$ and $W_2 = Span(S_2)$ be subspaces of a vector space V. Then $W_1 + W_2 = Span(S_1 \cup S_2)$
	\paragraph{Theorem 1.3.9} Let $W_1$ and $W_2$ be subspaces of a vector space V. Then $W_1 + W_2$ is also a subspace of V.
	\paragraph{Proposition 1.3.11} $W_1 + W_2$ is the smallest subspace containing $W_1 \cup W_2$: \newline
	Let $W_1$ and $W_2$ be subspaces of a vector space V and let W be a subspace of V such that $W_1 \cup W_2 \subseteq W$. Then $W_1 + W_2 \subseteq W$
	\paragraph{Remark} $W_1 \cup W_2$ is a subspace of V iff one is contained in another.
	
	
	
	\subsection{Linear Dependence and Linear Independence} 
	\paragraph{Definitions 1.4.2} Let V be a vector space, and let S be a subset of V.
	\begin{enumerate}
		\item A \ti{linear dependence} among the vectors of S is an equation
		$$a_1\vx_1 + \hdots + a_n\vx_n = \vo$$
		where the $x_i \in S$, and the $a_i \in \real$ are not all zero (i.e., at least one of the $a_i \neq \vo$
		\item the set S is said to be \ti{linearly dependent} if there exists a linear dependence among the vectors in S.
	\end{enumerate}
	
	\paragraph{Fact}
	Let S be a set. If $\vo \in S$, then S is dependent. 
	
	\paragraph{Definition 1.4.4} A subset S of a vector space V is \ti{linearly independent} if whenever we have $a_i \in \real$ and $\vx_i \in S$ such that $a_1\vx_1 + \hdots + a_n\vx_n = \vo$, then $a_i = 0$ for all $i$.
	
	\paragraph{Example}
	\textcolor{red}{In any vector space the empty subset $\emptyset$ is linearly independent.}
	\paragraph{Proposition 1.4.7}
	\begin{enumerate}
		\item Let S be a linearly independent subset of a vector space V, and let S' be another subset of V that \textcolor{blue}{contains} S. Then S' is also linearly dependent.
		\item Let S be linearly independent subset of a vector space V and let S' be another subset of V that is \textcolor{blue}{contained} in S. Then S' is also linearly independent.
	\end{enumerate}
	
	\subsection{Interlude on Solving Systems of Linear Equations (MAT223)}
	\subsection{Bases And Dimension (Jan 17)}
	\paragraph{Definition} A subset S of vector space V is called a \ti{basis} of V if V = Span(S) and S is linearly independent.
	\paragraph{Remark}
	\textcolor{red}{A basis is the maximal set of linearly independent vectors / minimal set of spanning vectors.}
	\paragraph{Examples} 
	\begin{enumerate}
		\item the standard basis S = \{\tb{$e_1$},...,\tb{$e_n$}\} in $\mb{R}^n$, since every vector $(a_1, ..., a_n) \in \mb{R}^n$ may be written as the linear combination $(a_1,..., a_n) = a_1e_1+ ... + a_ne_n$
		\item The vector space $\mb{R}^n$ has many other bases as well. e.g., in $\mb{R}^2$, consider the set $S = \{(1,2),(1,-1)\}$, which is l.i.
		\item Let $V = P_n(\mb{R})$ and consider $ S = \{1, x, x^2, ..., x^n\}$, which is a basis of V.
		\newline
		\proof It is clear that S spans V. For independence, consider
		$$a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n = \tb{0}$$
		Take the derivative of both sides,
		$$\frac{d^n}{dx^n}(a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n) = \frac{d^n}{dx^n}(0)$$
		$$n!a_n = 0 \implies a_n = 0$$
		Similarly, we have $a_i = 0$ for all $i$, as wanted.
		
		\item The empty subset, $\emptyset$, is a basis of the vector space consisting only of a zero vector, $\{\tb{0}\}$.
	\end{enumerate}
	\paragraph{Theorem 1.6.3} Let V be a vector space, and let S be a nonempty subset of V. Then S is a basis of V iff every vector $\tb{x} \in V$ may be written uniquely as a linear combination of the vectors in S. \newline
	\under{\ti{Proof:}}
	$\rightarrow:$ Assume  S is a basis of V, then given $\tb{x} \in V$, there are scalars $a_i \in \mb{R}$ and vectors $x_i \in S$ s.t. $\tb{x} = a_1x_1 + ... + a_nx_n$. To show this linear combination is unique, consider a possible second linear combination of vectors in S which also adds up to \tb{x}: $x= b_1x_1 + ...+b_nx_n$. Subtracting these two expressions for $\tb{x}$, we find that
	$$\tb{0} = a_1x_1 + ... + a_nx_n - (b_1x_1 + ...+b_nx_n)$$ 
	$$=(a_1 - b_1)x_1 + ...+(a_n-b_n)x_n$$
	\under{Since S is linearly independent}, the equation implies that $a_i = b_i$ for all $i$.\newline\newline
	$\leftarrow$: Assume every vector $\tb{x} \in V$ may be written uniquely as a linear combination of the vectors in S. This implies $Span(S) = V$. We must show that S is l.i. Consider an equation
	$$a_1x_1 +...+a_nx_n = \tb{0}$$
	Note that it is also the case that
	$$0\tb{x} = 0(x_1 + ...+x_n) = \tb{0}$$
	Since we assumed that every \tb{x} has a unique representation in S, then it must be true that $a_i = 0$ for all $i$. Hence S is l.i.
 	\paragraph{Theorem 1.6.6} Let V be a vector space that has a finite spanning set, and let S be a linearly independent subset of V. Then there exists a basis S' of V, with $S \subset S'$
	\paragraph{Lemma 1.6.8} Let S be a linearly independent subset of V and let $x \in V$, but $x \notin S$. Then $S \cup \{\tb{x}\}$ is l.i. iff $\tb{x} \notin Span(S)$.
	\paragraph{Insight} the number of vectors in a basis is, in a rough sense, a measure of ``how big" the space is.
	\paragraph{Theorem 1.6.10 (Basis Theorem)} Let V be a vector space and let S be a spanning set for V, which has m elements. Then no linearly independent set in V can have more than $m$ elements.\newline \newline
	\ti{\under{proof:}} It suffices to show that every set in V with more than m elements is linearly dependent. Write $S = {y_1, ..., y_m}$ and suppose $S' = {x_1, ..., x_n}$ is a subset of V with $n >m$ vectors. Consider an equation
	$$(1)a_1x_1 + ... + a_nx_n = \tb{0}$$
	Our goal is to show that $a_i$ not all 0.
	Since S spans V, there are scalars $b_{ij}$ s.t. for each $i$,
	$$x_i = b_{i1}y_1 + ... +b_{im}y_m$$
	Substituting these equations into (1), we get $$a_1(b_{11}y_1+...+b_{1m}y_m)+...+a_n(b_{n1}y_1+...+b_{nm}y_m)=\tb{0}$$
	Collecting terms and rearranging,
	$$(a_1b_{11}+...+a_nb_{n1})y_1+...+(a_1b_{1m}+...+a_nb_{nm})y_m = \tb{0}$$
	Since S is l.i., this is equivalent to solving the system
	$$b_{11}a_1+...+b_{n1}a_n = 0$$
	$$.$$
	$$.$$
	$$.$$
	$$b_{1m}a_1+...+b_{nm}a_n=0$$
	But this is a system with n unknowns and m equations and $n>m$, so there must exist a non-trivial solution $\{a_1,...,a_n\}$, which is what we wanted to show. \qed
	\paragraph{Corollary 1.6.11} Let $V$ be a vector space and let $S$ and $S'$ be two bases of $V$, with $m$ and $m'$ elements, respectively. Then $m = m'$.\newline \newline
	\proof\newline
	Since S is a spanning set of V and S' is l.i., we have $m'\leq m$. Since S' is a spanning set of V and S is l.i.m we have $m \leq m'$. Hence $m = m'$. \qed
	\paragraph{Definitions 1.6.12}
	\begin{enumerate}
		\item If V is a vector space with some finite basis(possibly empty), we say V is \under{\it{finite-dimentional}}.
		\item Let V be a finite-dimensional vector space. The dimension of V, denoted dim(V), is the number of vectors in a (hence any) basis of V.
		\item If $V = \{\tb{0}\}$, we define dim(V) = 0.
	\end{enumerate}
	\paragraph{Examples}
	\begin{enumerate}
		\item For each n, $\dim($\mb{R}^n$) = n$, since the standard basis contains n vectors.
		\item dim($P_n(\mb{R})) = n + 1$, since a basis for $P_n(\mb{R})$ contains n + 1 functions.
		\item The vector spaces $P(\mb{R}), C^1(\mb{R})$ and $C(\mb{R})$ are not finite-dimensional. We say that such spaces are \under{\it{infinite-dimentional}}.
		\item $\dim(Span\{(1,2,3),(4,5,6),(7,8,9)\}) = 2$
	\end{enumerate}
	\paragraph{\textcolor{blue}{Corollary 1.6.14}} Let W be a subspace of a finite-dimensional vector space V. Then $\dim(W) \leq \dim(V).$ Furthermore, $	\dim(W) = \dim(V)$ iff $W = V$.
	
	\paragraph{Corollary 1.6.15} Let W be a subspace of $\mb{R}^n$ defined by a system of homogeneous linear equations. Then dim(W) is equal to the number of free variables in the corresponding echelon form system.
	\paragraph{Theorem 1.6.18} Let $W_1$ and $W_2$ be finite-dimensional subspaces of a vector space V. Then $$\dim(W_1 + W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1 \cap W_2)$$
	\paragraph{Remark} Analogous to the Principle of Inclusion-Exclusion \newline \newline
	\proof Result obvious if either $W_1$ or $W_2$ is \{\tb{0}\}. \newline
	Therefore, we assume that neither $W_1$ nor $W_2$ is  \{\tb{0}\}. Starting from a basis S of $W_1 \cap W_2$. We can always find sets $T_1$ and $T_2$ (disjoint from S) such that $S \cup T_1$ is a basis for $W_1$ and $S \cup T_2$ is a basis for $W_2$. We claim that $U = S \cup T_1 \cup T_2$ is a basis for $W_1 + W_2$, since 
	$$ U = S \cup T_1 \cup T_2 = (S \cup T_1) \cup (S\cup T_2)$$
	$$Span(U) = Span((S \cup T_1) \cup (S\cup T_2)) = W_1 + W_2$$
	Next, prove that U is linearly independent. Any potential linear dependence among the vectors in U must have the form 
	$$\tb{v} + \tb{w}_1 + \tb{w}_2 = \tb{0}$$
	where $\tb{v} \in Span(S) = W_1 \cap W_2, \tb{w}_1 \in Span(T_1) \subset W_1, \tb{w}_2 \in Span(T_2) \subset W_2$. (slice the linear combination into the sum of the vectors from 3 vector spaces). It suffices to prove that in any such potential linear dependence, we must have $\tb{v} = \tb{w}_1 = \tb{w}_2 = \tb{0}$ (each vector is a lin comb, and equals \tb{0}).  \newline
	Consider $\tb{w}_2 = -\tb{v} - \tb{w}_1$. Since $-\tb{v} - \tb{w}_1 \in W_1, \tb{w}_2 \in W_2,$ we must have $\tb{w}_2 \in W_1 \cap W_2$. By definition, $\tb{w}_2 \in Span(T_2)$ But $S \cap T_2 = \emptyset$, hence $Span(S) \cap Span(T_2) = \{\tb{0}\}.$ Therefore we must have $\tb{w}_2 = \tb{0}$. So then $-\tb{v} = \tb{w}_1 \in W_1 \cap W_2$. Since $S \cap T_1 = \emptyset$, $Span(S) \cap Span(T_1) = \{\tb{0}\}$ and we have $\tb{w}_1 = \tb{0}$, so $\tb{v} = \tb{0}$ as well. So U is independent. \newline
	\begin{align*}
		|U| &= |S| + |T_1| + |T_2|\\
		&=\dim{W_1\cap W_2} + (\dim{W_1}-\dim{W_1\cap W_2}) + (\dim{W_2} - \dim{W_1\cap W_2})\\
		&= \dim(W_1) + \dim(W_2) - \dim(W_1 \cap W_2)\\
	\end{align*}
	\paragraph{Exercises for 1.4}
	1.(k), 7
	\paragraph{Exercises for 1.6}
	1.(d)(e)(f), 3, 4, 16
	\newpage
	\section{Linear Transformations}
	\subsection{Linear Tranformations}
	A function T from V to W is denoted by $T: V \rightarrow W$. The vector $\tb{w} = T(\tb{v})$ in W is called the \ti{image} of \tb{v} under the function T. Loosely speaking, we want our functions to turn the algebraic operations of addition and scalar multiplication in V into addition and scalar multiplication in W.
	\paragraph{Definition 2.1.1} A function $T: V \rightarrow W$ is called a \ti{linear mapping} or a \ti{linear transformation} if it satisfies
	\begin{enumerate}
		\item $T(\tb{u} + \tb{v}) = T(\tb{u}) + T(\tb{v})$ for all \tb{u} and \tb{v} $\in$ V
		\item $T(a\tb{v}) = aT(\tb{v})$ for all $a \in \mb{R}$ and $\tb{v} \in V$
	\end{enumerate}
	V is called the \ti{domain} of T and W is called the \ti{target} of T. \newline
	We say that a linear transformation preserves the operations of addition and scalar multiplication.
	\paragraph{Property} A linear mapping always takes the zero vector in the domain vector space to the zero vector in the target vector space.
	\paragraph{Proposition 2.1.2} A function \litran is a linear transformation if and only if for all $a$ and $b \in \mb{R}$ and all $\tb{u}$ and $\tb{v} \in V$
	$$T(a\tb{u} + b\tb{v}) = aT(\tb{u}) + bT(\tb{v})$$
	\paragraph{Corollary 2.1.3} A function \litran is a linear transformation if and only if for all $a_1, .., a_k \in \mb{R}$ and for all $\tb{v}_1, ..., \tb{v}_k \in V$:
	$$T(\sum^k_{i = 1} a_i \tb{v}_i) = \sum^k_{i = 1}a_iT(\tb{v}_i)$$
	\paragraph{Examples}
	\begin{enumerate}
		\item Let V be any vector space, and let W = V. The \ti{under{identity transformation}} $I: V \rightarrow V$ is defined by I(\tb{v}) = \tb{v} for all \tb{v} $\in$ V.
		\item Let V and W be any vector spaces, and let \litran be the mapping that takes every vector in V to the zero vector in W:
		$$ T(\tb{v}) = \tb{0}_W$$
		for all \tb{v} $\in$ V. T is called \it{\under{zero transformation}}.
		\item $T(\tb{x}) = a_1x_1 + ... + a_nx_n$
		\item Differentiation, definite integration
	\end{enumerate} 
	\paragraph{Remark} The inner product plays a crucial role in linear algebra in that it provides a bridge between algebra and geometry, which is the heart of the more advanced material that appears later in the text.
	\paragraph{Proposition 2.1.14} If \litran is a linear transformation and V is finite-dimensional, then T is uniquely determined by its values on the members of a basis of V. \newline
	\proof Show that if S and T are linear transformations that take the same values on each member of a basis for V, then in fact S = T. \newline
	\begin{align*}
		T(v) &= T(a_1v_1 + ... + a_kv_k)\\
		&= a_1T(v_1) + ... + a_kT(v_k)\\
		&= a_1S(v_1) + ... + a_kS(v_k)\\
		&= S(a_1v_1 + ... + a_kv_k)\\
		&= S(v)
	\end{align*}
	Therefore, S and T are equal as mappings from V to W. \qed
	\subsection{Linear Transformations Between Finite-Dimensional Vector Spaces}
	\paragraph{Proposition 2.2.1} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W. If $\{\tb{v}_1,...,\tb{v}_k\}$ is a basis for V and $\{\tb{w}_1,...,\tb{w}_l\}$ is a basis for W, then \litran is uniquely determined by the $l\cdot k$ scalars used to express $T(\tb{v}_j), j = 1,...,k$, in terms of $\tb{w}_1, ..., \tb{w}_l$.
	\paragraph{Definition 2.2.6} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W, and let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$, respectively, be any bases for V and W. Let $a_{ij}, 1 \leq i \leq l$ and $1 \leq j\leq k$ be the $l \cdot k$ scalars that determine T with respect to the bases $\alpha$ and $\beta$. The matrix whose entries are the scalars $a_{ij}$, $1 \leq i \leq l$ and $1 \leq j\leq k$, is called the {\ti{matrix of the linear transformation T with respect to the bases $\alpha$ for V and $\beta$ for W}. This matrix is denoted by $[T]^\beta_\alpha$.
	\paragraph{Remark} The basis vectors in the domain and target spaces are written in some particular order.
	\paragraph{Definition of coordinate vectors} If $\tb{v} = a_1\tb{v}_1+...+a_k\tb{v}_k$ and $\tb{w} = b_1\tb{w}_1+...+b_l\tb{w}_l$, we can express \tb{v} and \tb{w} in coordinates, respectively, as a $k \times 1$ matrix and as an $l \times 1$ matrix, with respect to the chosen bases. These coordinate vectors will be denoted by $[\tb{v}]_\alpha$ and $[\tb{w}]_\beta$, respectively. Thus
	$[\tb{v}]_\alpha = \begin{bmatrix} 
a_1 \\
\vdots\\
a_k
\end{bmatrix}$
and
	$[\tb{w}]_\beta = \begin{bmatrix} 
b_1 \\
\vdots\\
b_l
\end{bmatrix}$
\paragraph{Proposition 2.2.15} Let \litran be a linear transformation between vector spaces V of dimension k and W of dimension $l$. Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W. Then for each $\tb{v} \in V$, $$[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$$
\proof Let $\tb{v} = x_1\tb{v}_1 + ...+x_k\tb{v}_k \in V$. Then if $T(\tb{v}_j) = a_{1j}\tb{w}_1 + ...+ a_{lj}\tb{w}_l$
\begin{align*}
	T(\tb{v}) &= \sum_{j=1}^kx_jT(\tb{v}_j)\\
	&=\sum_{j=1}^kx_j(\sum_{i=1}^la_{ij}\tb{w}_i)\\
	&=\sum_{i=1}^l(\sum_{j=1}^kx_ja_{ij})\tb{w}_i
\end{align*}
Thus, the $i$th coefficient of T(\tb{v}) in terms of $\beta$ is $\sum_{j=1}^kx_ja_{ij}$ and 
$[T(\tb{v})]_\beta = \begin{bmatrix}
	\sum_{j=1}^kx_ja_{1j}\\
	\vdots\\
	\sum_{j=1}^kx_ja_{lj}
\end{bmatrix}$ which is precisely $[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$. \qed

\paragraph{Proposition 2.2.19} 
 Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W, and let $\tb{v} = x_1\tb{v}_1 +...+x_k\tb{v}_k \in V$
 \begin{enumerate}
 	\item If A is an $l \times k$ matrix, then the function $$T(\tb{v}) = \tb{w}$$
 	where $[\tb{w}]_\beta = A[\tb{v}]_\alpha$ is a linear transformation.
 	\item If $A = [S]_\alpha^\beta$ is the matrix of a transformation $S: V \rightarrow W$, then the transformation T constructed from $[S]_\alpha^\beta$ is equal to S.
 	\item If T is the transformation of (1) constructed from A, then $[T]_\alpha^\beta = A$
 \end{enumerate}
 
 \paragraph{Proposition 2.2.20} Let V and W be finite-dimensional vector spaces. Let $\alpha$ be a basis for V and $\beta$ a basis for W. Then the assignment of a matrix to a linear transformation from V to W given by T goes to $[T]_\alpha^\beta$ is injective and surjective.
 \paragraph{Notes}
 \begin{enumerate}
 	\item When proving a function T is not a linear transformation, can consider $T(\tb{0}) \neq \tb{0}$.
 \end{enumerate}
 \subsection{Kernel and Image}
 \paragraph{Definition 2.3.1} The $kernel$ of T, denoted $Ker(T)$, is the subset of V consisting of all vectors $\tb{v} \in V$ such that $T(\tb{v}) = 0$.
 \paragraph{Remark} Kernel is different from null spaces. A null space is about a matrix, and it is something in $\real ^n$.
 \paragraph{Proposition 2.3.2} Let \litran be a linear transformation. Ker(T) is a subspace of V.
 \paragraph{Examples}
 \begin{enumerate}
 	\item Let $V = P_3(\mb{R})$. Define $T: V \rightarrow V$ by $T(p(x)) = \frac{d}{dx}p(x)$. Ker(T) only consists constant polynomials.
 	\item Let $V = W = \mb{R}^2$. Let T be a rotation $R_\theta$. Then $Ker(T) = \{\tb{0}\}$.
 \end{enumerate}
 \paragraph{Proposition 2.3.7} \slitran of finite-dimensional vector spaces, and let $\alpha$ and $\beta$ be bases for V and W, respectively. Then $\tb{x} \in Ker(T)$ ife lf the coordinate vector of \tb{x}, $[\tb{x}]_\alpha$, satisfies the system of equations
 $$ a_{11}x_1+...+a_{1k}x_k = 0$$
 $$\vdots$$
 $$ a_{l1}x_1+...+a_{lk}x_k = 0$$
 where the coefficients $a_{ij}$ are the entries of the matrix \mt.
\paragraph{Remark} This says $$x \in \ker(T) \iff [x]_\alpha \in Nul[T]_\alpha^\beta$$
\paragraph{Proposition 2.3.8 Independence is Basis-Independent} Let V be a finite-dimensional vector space, and let $\alpha = \{\tb{v}_1, ..., \tb{v}_k \}$ be a basis for V. Then the vectors $\tb{x}_1,...,\tb{x}_m \in V$ are linearly independent iff their corresponding coordinate vectors $[\tb{x}_1]_\alpha, ..., [\tb{x}_m]_\alpha$ are linearly independent.
\paragraph{Definition 2.3.10} The subset of W consisting of all vectors $\tb{w} \in W$ for which there exists a $\tb{v} \in V$ such that $T(\tb{v}) = \tb{w}$ is called the $image$ of T and is denoted by $Im(T).$

\paragraph{Proposition 2.3.11} Let \litran be a linear transformation. The image of T is a subspace of W.

\paragraph{Proposition 2.3.12} If $\{ \tb{v}_1, ..., \tb{v}_m\}$is any set that spans V (in particular, it could be a basis of V), then $\{T(\tb{v}_1),...,T(\tb{v}_m)\}$ spans $Im(T)$.
\paragraph{Corollary 2.3.13} If $\alpha = \{ \tb{v}_1, ..., \tb{v}_k\}$ is a basis for V and $\beta = \{ \tb{w}_1, ..., \tb{w}_l\}$ is a basis for W, then the vectors in W, whose coordinate vectors (in terms of $\beta$) are the columns of $[T]_\alpha^\beta$, span $Im(T)$.
\paragraph{Rank-Nullity Theorem 2.3.17} If V is finite-dimensional vector space and \litran is a linear transformation, then
$$\dim(Ker(T)) + \dim(Im(T)) = \dim(V)$$
Equivalently, 
$$Nullity(T) + Rank(T) = \dim(V)$$


\subsection{Applications of the Dimension Theorem}
\paragraph{Proposition 2.4.2} A linear transformation \litran is injective iff $\dim(Ker(T)) = 0$, or $\dim(Im(T)) = \dim(V)$.
\paragraph{Remark} Analogously, in MAT223 we said that \textcolor{red}{a matrix is one-to-one if all the columns are l.i.}.
\paragraph{Corollary 2.4.3} A linear mapping \litran on a finite-dimensional vector space V is injective iff $\dim(Im(T)) = \dim(V)$.
\paragraph{Corollary 2.4.4} If $\dim(W) < \dim(V)$ and \litran is a linear mapping, then T is not injective. \newline
\begin{proof}
	$$dim(Im(T)) \leq dim(W) < dim(V)$$
	$$\implies dim(Ker(T)) > 0$$
\end{proof}
\paragraph{Proposition 2.4.7} If W is finite-dimensional, then a linear mapping \litran is surjective iff $\dim(Im(T)) = \dim(W)$
\paragraph{Remark} Analogously, in MAT223 we said that \textcolor{red}{a matrix is onto if there is a pivot in every row}.
\paragraph{Corollary 2.4.8} If V and W are finite-dimensional, with $\dim(V) < \dim(W)$, then there is no surjective linear mapping \litran \newline
\begin{proof}
	$\dim(Im(T)) \leq \dim(V) < \dim(W) \implies T$ is not surjective
\end{proof}
\paragraph{Corollary 2.4.9} A linear mapping \litran can be surjective iff $$\dim(V) \geq \dim(W)$$
\paragraph{Proposition 2.4.10} Let $\dim(V) = \dim(W)$. A linear transformation \litran is injective iff it is surjective.
\paragraph{Proposition 2.4.11} Let \litran be a linear transformation, and let $w \in Im(T)$. Let $v_1$ be any fixed vector with $T(v_1) = w$. Then every vector $v_2 \in T^{-1}(\{w\})$ can be written uniquely as $v_2 = v_1 + u$, where $u \in Ker(T)$
\paragraph{Remark} In this situation $T^{-1}(\{w\})$ is a subspace of V iff $w = 0$.
\paragraph{Corollary 2.4.15} Let \litran be a linear transformation of finite-dimensional vector spaces, and let $w \in W$. Then there is a unique vector $v \in V$ such that $T(v) = w$ iff
\begin{enumerate}
	\item $w \in Im(T)$, and
	\item dim(Ker(T)) = 0
\end{enumerate}
\paragraph{Proposition 2.4.16} With notation as before
\begin{enumerate}
	\item The set of solutions of the system of linear equations $A\tb{x} = \tb{b}$ is the subset $T^{-1}(\{\tb{b}\})$ of $V = \real^n$
	\item The set of solutions of the system of linear equations $A\tb{x} = \tb{b}$ is a subspace of V iff the system is homogeneous, in which case the set of solutions is $Ker(T)$.
\end{enumerate}
\paragraph{Corollary 2.4.17}
\begin{enumerate}
	\item The number of free variables in the homogeneous system $A\tb{x} = \tb{0}$ (or its echelon form equivalent) is equal to $\dim(Ker(T))$
	\item The number of basic variables of the system is equal to $dim(Im(T)$
\end{enumerate}
\paragraph{Definition 2.4.18} Given an inhomogeneous system of equations, $A\tb{x} = \tb{b}$, any single vector $\tb{x}$ satisfying the system (necessarily $\tb{x} \neq \tb{0}$ is called a \under{particular solution} of the system of equations.
\paragraph{Proposition 2.4.19} Let $\tb{x}_p$ be a particular solution of the system $A\tb{x} = \tb{b}$. Then every other solution to $A\tb{x} = \tb{b}$ is of the form $\tb{x} = \tb{x}_p + \tb{x}_h$, where $\tb{x}_h$ is a solution of the corresponding homogeneous system of equations $A\tb{x} = \tb{0}$. Furthermore, given $\tb{x}$ and $\tb{x}_p$, there is a unique
	$\tb{x}_h$ such that $\tb{x} = \tb{x}_p + \tb{x}_h$.
\paragraph{Corollary 2.4.20} The system $A\tb{x} = \tb{b}$ has a unique solution iff $\tb{b} \in Im(T)$ and the only solution to $A\tb{x} = \tb{0}$ is the zero vector.
\end{document}

\documentclass[11pt]{article}
% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}
\usepackage{esint}

% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\bx}[0]{\mathbf{x}}
\newcommand{\bv}[0]{\mathbf{v}}
\newcommand{\bw}[0]{\mathbf{w}}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\func}[3]{\tb{#1}: {#2} \rightarrow {#3} }
\newcommand{\vx}[0]{\tb{x}}
\newcommand{\vy}[0]{\tb{y}}
\newcommand{\vz}[0]{\tb{z}}
\newcommand{\vo}[0]{\tb{0}}
\newcommand{\va}[0]{\tb{a}}
\newcommand{\vb}[0]{\tb{b}}
\newcommand{\vc}[0]{\tb{c}}
\newcommand{\ve}[0]{\tb{e}}
\newcommand{\vm}[0]{\tb{m}}
\newcommand{\vh}[0]{\tb{h}}
\newcommand{\vf}[0]{\tb{F}}
\newcommand{\vi}[0]{\tb{i}}
\newcommand{\vj}[0]{\tb{j}}
\newcommand{\vk}[0]{\tb{k}}
\newcommand{\vg}[0]{\tb{G}}
\newcommand{\vn}[0]{\tb{n}}
\newcommand{\vu}[0]{\tb{u}}
\newcommand{\vL}[0]{\tb{L}}
\newcommand{\ff}[0]{\tb{f}}
\newcommand{\fg}[0]{\tb{g}}
\newcommand{\rational}[0]{\mathbb{Q}}
\newcommand{\p}[0]{\partial}
\newcommand{\qed}[0]{$\hfill\blacksquare$}
\newcommand{\qerat}{\tag*{$\blacksquare$}}
\newcommand{\lima}{\underset{\vx \rightarrow \va}{\lim}}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}


% Attr.
\title{Methods of Data Analysis \\ -- STA302 Course Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage
\section{May 7th - Introduction, p-values and statistical significance}
\paragraph{Definition 1.1 - Statistical Analysis} 
Data Analysis that relies on Probability theory to account for the variability of the data.
\paragraph{Permutation Test 1.2}
Insert random premise, observe two samples Group A and Group B. \\
If the groups have no effect, all of the permutations are equally likely. \\
We can plot the Permutation Distribution with respect to difference between sample means.
\paragraph{Characteristics of Permutation Test 1.3}
\begin{enumerate}
	\item Involves simple probability theory
	\item distribution-free
	\item listing all the permutation for large dataset is almost impossible
\end{enumerate}

\paragraph{Definition 1.4 - Statistical Significance}
We say a difference is \textbf{statistically significant} if it's less probable than our pre-determined significance level. (when p-value $p <$  significance level $\alpha$)
\paragraph{Definition 1.5 - Significant Effect} We say the groups have a \textbf{significant effect} if it causes the variable of interest to be significantly different.

\section{May 9th - Hypothesis testing, t-test and ANOVA}
\subsection{The basics}
\paragraph{Fact 2.1.1} If $H_0$ is true, the p-value $\sim U(0,1)$ \\\\
\under{\ti{remarks}}: This is saying that if p-value is greater than significance level, then it does not say anything about our confidence, it's just a value. Proof can be found online.
\paragraph{Tradeoff Between Type I and Type II Error}
It's common to fix $\alpha$ (significance level or type-I error) and minimize type-II error.
\subsection{t-test}
Under the model $(y_i|X=xi) \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$, an unbiased estimator of $\sigma^2$ is 
$$S^2 = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-2} $$
Then
$$ \frac{\hat{\beta_1} - \beta_1}{\sqrt{\frac{S^2}{SXX}}} \sim t_{(n-2)}$$


\subsection{One-way Analysis of variance (ANOVA)}
Suppose the response $Y$ is quantitative and the predictor $X$ is categorical, taking $t$ values or levels denoted $1, \hdots, t$. With the regression model, we assume that the only aspect of the conditional distribution of $Y$, given $X = x$, that changes as $x$ changes, is the mean. \\
Suppose we are interested in assessing whether or not there is a relationship between the response and the predictor. There is no relationship if and only if all the conditional distributions are the same. This is true under our assumptions if and only if all the means are equal.
In our case, one-way ANOVA is an extension of the t-test to 3 or more samples focus analysis on group differences.\\
$H_0$: All groups are the same.
If groups are different, we expect there is a bigger difference between groups (\textcolor{blue}{the group effect}) than within groups (\textcolor{blue}{natural variability of the data}).
\paragraph{Basic Definitions}
Suppose we have $T$ groups and $n_t$ observations for the $t$-th group, and we denote each observation as $y$.
\begin{enumerate}
	\item \under{SST}: This is the sum of the squared deviations between each observation and the overall mean:
	$$SST = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y})^2$$
	\item \under{SSE}: This is the sum of the squared deviations between each observation and the mean of the group to which it belongs:
	$$SSE = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y_t})^2$$
	\item \under{SSG}: This is the sum of the squared deviations between each group mean and the overall mean:
	$$SSG = \sum_{t=1}^T\sum_{i=1}^{n_t}(\bar{y_t}-\bar{y})^2$$
\end{enumerate}

\paragraph{Sum of Squares Decomposition}
Total sum of squares = Within group sum of squares + Between group sum of squares \\
$$\sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y})^2 = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y_t})^2 + \sum_{t=1}^T\sum_{i=1}^{n_t}(\bar{y_t}-\bar{y})^2$$
In shorthand:
$$SST = SSE + SSG$$
\proof\\
add $-\bar{y_t}+\bar{y_t}$ inside the squared error term and everything is just like a short proof in STA261, nothing interesting. \qed
\paragraph{ANOVA}
We want to assess how large is SSG relative to SSE, but it would be hard to establish a distribution for SSG/SSE. Knowing a sum of squares divided by its degrees of freedom has a chi-square distribution, we can conclude that
$$SSG/(T-1) \sim \chi_{T-1}^2, \quad SSE/(n-T) \sim \chi_{n-T}^2$$
\paragraph{Theorem 2.2.1} If $SSG/(T-1) = SSE/(n-T)$, then $\frac{SSG/(T-1)}{SSE/(n-T)} \sim F_{T-1, n-T}$ \\\\
\proof \\
In STA261, we've proven that if $\sigma_x^2 = \sigma_y^2$, then $\frac{\hat{\sigma}_x^2}{\hat{\sigma}_y^2} \sim F_{n-1,n-1}$.\\ Since SSG/(T-1) is an estimation for the variation between groups ($\sigma_T$) and $SSE/(n-T)$ is an estimation for the variation within groups ($\sigma_\varepsilon$), then the result follows. \qed
\paragraph{Remarks 2.2.2} Thus a small p-value indicates theses variances are different, which is evidence for the existence of some group effect.
\paragraph{Theorem 2.2.3 One-way ANOVA Table} if p-value $< \alpha$, we reject $H_0$: groups have no effect. \\
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Source & Sum of Squares & df & Mean Squares & Test Statistic \\
		\hline
		Between & SSG & $T-1$ & $MSG = \frac{SSG}{T-1}$ & $F = \frac{MSG}{MSE}$ \\
		\hline
		Within & SSE & $n - T$ & $MSE = \frac{SSE}{n - T}$ &\\
		\hline
		Total & $SST$ & $ n -1$ &&\\
		\hline
	\end{tabular}
\section{May 14th - Linear Regression: Least Square Error Formulation}
\subsection{Matrix Notation}
$\vx = \begin{bmatrix}
	x_1 \\ x_2
\end{bmatrix}$ is a random variable. \\In addition, $A = \begin{bmatrix}
	a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
\end{bmatrix}$, $\vc = \begin{bmatrix}
	c_1\\c_2\\c_3
\end{bmatrix}$.\\
Then $$E[\vx] = \mu = \begin{bmatrix}
	\mu_1 \\ \mu_2
\end{bmatrix}$$
$$Var[\vx] = \Sigma = \begin{bmatrix}
	\sigma^2_1 & \sigma^2_{12} \\ \sigma^2_{12} & \sigma^2_2
\end{bmatrix}$$
where $\sigma^2_{ij} = cov(x_i, x_j)$.
\paragraph{Theorem 3.1.1}
Let $\vz = A\vx + \vc$. Then $$E[\vz] = A\mu + \vc$$
$$Var[\vz] = A\Sigma A^T$$
\subsection{Linear Regression}
We have a vector of $n$ predictors $\vx = [x_1,\hdots,x_n]$, as well as $n$ associated response variables $\vy = [y_1, \hdots, y_n]$. We want to estimate the parameters $\beta_0$ and $\beta_1$ that best fit the model $y = \beta_0 + \beta_1 x$. (In matrix notation: $\vy = X\beta$ where $\vy = \begin{bmatrix}
	y_1 \\ \vdots \\ y_n \end{bmatrix}, X = \begin{bmatrix}
		1 & x_1 \\ \vdots & \vdots \\ 1 & x_n
	\end{bmatrix}, \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}$).
\subsubsection{Least Square Estimation}
Minimize sum of squared errors (MSE): $$\sum_{i=1}^n (\beta_0 + \beta_1 x_i - y_i)^2$$
We take derivative of $\sum_{i=1}^n (\vy - X\hat{\beta})^2$ wrt $\hat{\beta}$, set this to 0 and get 
\paragraph{Theorem 3.2.1.1}$$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
$$\hat{\vy} = X\hat{\beta} = X(X^TX)^{-1}X^T\vy$$
\paragraph{Remark 3.2.1.2}
$X(X^TX)^{-1}X^T$ is called \under{the hat matrix} since it puts the hat on $\vy$. This matrix $(H)$ has the following properties:
\begin{enumerate}
    \item $H^T = H$
    \item $HH = H$
    \item $HX = X$
\end{enumerate}
\subsubsection{ANOVA}
Estimate how good a linear regression model is.
\paragraph{Basic Definitions}
$\bar{y}$ is called base estimation.
\begin{enumerate}
	\item \under{SST}: This is the sum of the squared deviations between each observation and the mean:
	$$SST = \sum_{i=1}^n(y_{i}-\bar{y})^2$$
	\item \under{SSE}: This is the sum of the squared deviations between each observation and the corresponding prediction
	$$SSE = \sum_{i=1}^n(\hat{y_i} - y_i)^2$$
	\textcolor{blue}{unexplained variation: How much our explanation is away from the true observation?}
	\item \under{SSG}: This is the sum of the squared deviations between each prediction and the mean.
	$$SSG = \sum_{i=1}^n(\hat{y_i}-\bar{y})^2$$
	\textcolor{blue}{explained variation: How much our explanation takes us away from the base prediction?}
\end{enumerate}
\paragraph{Coefficient of Determination}
$$R^2 = \frac{SSG}{SST} = 1- \frac{SSE}{SST}$$
($0 \leq R^2 \leq 1$)\\
The closer $R^2$ is from 1, the better the fit is.
\section{May 16th - Linear Regression: Maximum Likelihood Formulation}
\subsection{the Linear Regression Model}
\paragraph{Definition 4.1.1}
The \under{best linear unbiased estimator (BLUE)} is the unbiased estimator with the lowest variance.
\paragraph{Gauss-Markov Assumptions 4.1.2}
If $E[e_i] = 0, Cov(e_i, e_j) = 0 \, \forall i \neq j$ and $Var(e_i) = \sigma^2 < \infty \, \forall i$, then \textcolor{red}{the best linear unbiased estimator for $\beta$'s are given by minimizing the MSE}
\paragraph{the Linear Regression Model}
$$y_i = \beta_0 + \beta_1x_i + e_i$$
where $e_i$ is a random variable that represents the residual. \\
\paragraph{Assumptions}
\begin{enumerate}
    \item $e_i \overset{i.i.d.}{\sim} N(0, \sigma^2)$ which follows the Gauss-Markov assumptions. 
    \item $(\vy|\vx) \sim N(X\beta, I\sigma^2)$ or $(Y|X = x) \sim N(\beta_0 + \beta_1x, \sigma^2)$
\end{enumerate}
\subsection{Maximum Likelihood Estimation}
$$l(\beta | \vx) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} (\vy - X \beta)^T(\vy - X\beta)$$
Maximizing this term wrt $\beta$ is equivalent to minimizing $(\vy - X\beta)^T(\vy-X\beta)$, which gives $$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
\paragraph{Corollary 4.2.1}
Minimizing MSE and the likelihood function leads to the same estimate $\hat{\beta}$.
\paragraph{A Biased Estimator of $\sigma^2$}
$$l(\beta | \vx) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} (\vy - X \beta)^T(\vy - X\beta)$$
Maximizing the likelihood function wrt to $sigma^2$:
$${\hat{\sigma}}^2 = \frac{(\vy - X\hat{\beta})^T(\vy-X\hat{\beta})}{n} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}$$
This is MSE, a biased estimator of $\sigma^2$. \\
The unbiased estimator of $\sigma^2$ is \textcolor{red}{$$\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-2}$$}
\subsection{Inference}
The action of extracting information about parameters given a dataset.\\
\paragraph{Mean and Variance of $\vy$}
Since $\vy \sim N(X\beta, I\sigma^2$, then $E[\vy] = X\beta$ and $Var[\vy] = I\sigma^2$.
\paragraph{Mean and Variance of $\hat{\beta}$}
\begin{align*}
    E[\hat{\beta}] &= E[(X^TX)^{-1}X^T\vy]\\
    &= (X^TX)^{-1}X^TE[\vy]\\
    &= (X^TX)^{-1}X^TX\beta \\
    &= \beta
\end{align*}
$\implies$ $\hat{\beta}$ is an unbiased estimator of $\beta$.
\begin{align*}
    Var[\hat{\beta}] &= Var[(X^TX)^{-1}X^T\vy]\\
    &= (X^TX)^{-1}X^T \, Var[\vy|X]((X^TX)^{-1}X^T)^T\\
    &= (X^TX)^{-1}X^T \, I\sigma^2 ((X^TX)^{-1}X^T)^T \\
    &= \sigma^2 (X^TX)^{-1}X^T((X^TX)^{-1}X^T)^T \\
    &= \sigma^2 (X^TX)^{-1}X^T(X((X^TX)^{-1})^T)\\
    &= \sigma^2 (X^TX)^{-1}X^T(X((X^TX)^{T})^{-1})\\
    &= \sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}\\
    &= \sigma^2 (X^TX)^{-1}
\end{align*}
\paragraph{Theorem 4.3.0.1} $$\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$$
\proof\\
$\hat{\beta} = (X^TX)^{-1}X^T\vy$, so $\hat{\beta}$ is a linear combination of normal r.v.'s($y_i$'s), therefore $\hat{\beta}$ follows normal distribution with mean and variance we have calculated. \qed
\subsubsection{Inference for $\beta_1$}
$$\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{SSX})$$
where $SSX = \sum_{i=1}^n(x_i - \bar{x})^2$
Then $$\frac{\hat{\beta_1} - \beta_1}{\sigma/\sqrt{SSX}} \sim N(0,1)$$
\paragraph{Theorem 4.3.1.1}
$$\frac{(n-2)S^2}{\sigma^2} \sim \chi^2_{(n-2)}$$
where $S^2 = \frac{1}{n-2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)^2$
\paragraph{Theorem 4.3.1.2}
$$\frac{\hat{\beta_1} - \beta_1}{S/\sqrt{SSX}} \sim t_{n-2}$$
where $SSX = \sum_{i=1}^n(x_i - \bar{x})^2$
\paragraph{Model Checking}
$H_0$: $\beta_1 = 0$
Then under $H_0$, Theorem 4.3.1.2 applies. 
\begin{enumerate}
    \item If the p-value is small, then $\vy$ and $\vx$ are statistically significant.
    \item 0.95 confidence level for $\beta_1$:
    $$(\hat{\beta_1} - t_{(n-2)(1-\frac{\alpha}{2})}\frac{S}{\sqrt{SSX}}, \hat{\beta_1} + t_{(n-2)(1-\frac{\alpha}{2})}\frac{S}{\sqrt{SSX}})$$
\end{enumerate}
\subsubsection{Inference for $\beta_0$}
$$\hat{\beta_0} \sim N(\beta_0, \sigma^2\frac{\sum x_i^2}{n\,SSX})$$

\section{May 23th - Diagnostic for the linear regression model}
\paragraph{Review of the model}
$$y_i = \beta_0 + \beta_1 x_i + e_i$$ where $e_i \sim N(0, \sigma^2)$
$$\vy = X\beta + \ve \implies \vy|X \sim N(X\beta, I\sigma^2)$$
where $$\hat{\beta}_{MLE} = (X^TX)^{-1}X^T\vy$$
$$\hat{\beta} \sim N(\beta, \sigma^2(X^TX)^{-1})$$
\paragraph{Notes}
\textcolor{red}{In all cases, we have $\sum_{i=1}^n e_i$ = 0.}

\subsection{Predictive Inference}
Since $\hat{\vy} = X\hat{\beta}$, then
$$\hat{\vy} \sim N(X\beta, \sigma^2X(X^TX)^{-1}X^T)$$
\paragraph{Prediction}
For a new, unobserved predictor $x^*$, a simple prediction for the response could be $$y^* = \beta_0 + \beta_1x^*$$
\paragraph{Predictive Distribution}
We have $\hat{\beta} = [\hat{\beta}_0, \hat{\beta}_1]^T$, then
$$\hat{y}^* = \hat{\beta}_0 + \hat{\beta}_1x^*$$
Matrix notation:
$$\hat{\vy}^* = X^*\hat{\beta}$$
where $X^*$ is a vector of new observation $\vx^*$ added a column of 1s. \\
then
$$\hat{\vy}^* \sim (X^*\beta, \sigma^2X^*(X^TX)^{-1}X^{*T})$$
\paragraph{Confidence Interval for $X^*\beta$}
$$\frac{\hat{\vy}^*-X^*\beta}{\sigma\sqrt{X^*(X^TX)^{-1}X^{*T}}} \sim N(0,I)$$
Then $$0.95 CI = \hat{y}^*_i \pm 1.96 * \sigma \sqrt{[X^*(X^TX)^{-1}X^{*T}]_{ii}}$$
\paragraph{Remarks}
The confidence interval reflects our uncertainty about the population regression line
\paragraph{the Prediction Error}
$$y^* - \hat{y}^* = \beta_0 + \beta_1x^* + \varepsilon^* - \hat{\beta}_0 + \hat{\beta}_1x^*$$
Matrix notation:
$$\vy^* - \hat{\vy}^* = X^*\beta + \varepsilon - X^*\hat{\beta}$$
Distribution:
$$\vy^* - \hat{\vy}^* \sim N(\mu, \Sigma)$$ where
\begin{align*}
    \mu &= E[X^*\beta + \varepsilon - X^*\hat{\beta}]\\
    &= X^*\beta + E[\varepsilon] - E[X^*\hat{\beta}]\\
    &= X^*\beta + 0 - X^*\beta \\
    &= 0 \\
    \Sigma &= Var[X^*\beta + \varepsilon - X^*\hat{\beta}] \\
    &= Var[\varepsilon - X^*\hat{\beta}]\\
    &= Var[\varepsilon] + Var[X^*\hat{\beta}] + 2Cov[\varepsilon, X^*\hat{\beta}] \\
    &= \sigma^2I + \sigma^2X^*(X^TX)^{-1}X^{*T} + 0\\
    &= \sigma^2[I + X^*(X^TX)^{-1}X^{*T}]
\end{align*}
Then we can construct a CI for $\hat{\vy}^*$ using t-distribution ($df = n-2$)
\subsection{Checking the Model Assumption}
We will divide model checking into 3 pieces:
\begin{enumerate}
    \item Error assumption ($e_i \overset{i.i.d}{\sim} N(0,  \sigma^2)$)
    \item Identical distribution (checking for unexpected observations)
    \item Model assumption (linearity)
\end{enumerate}
\subsubsection{Checking Error Assumption}
We only have access to the residuals (observed errors)
$$\hat{e}_i = y_i - \hat{y}_i$$
$$\hat{\ve} = (I - H)\vy$$
\paragraph{Constant variance}
Check $Var(e_i) = \sigma^2 \, \forall i$\\
Plot the residuals against the fitted values($\hat{y}_i$)
% \paragraph{Zero mean/ Independence}
% Plot the residuals against the predictors, see if clustered around zero and looks random
\paragraph{Normality of residuals}
Quantile to Quantile plot (QQplot)
\paragraph{Uncorrelatedness / Independence}
\begin{enumerate}
    \item Scatter plot (y against x)
    \item Residual plot against predictors (see if clustered around zero and looks random)
    \item Residual Sequence plot
\end{enumerate}
\paragraph{Remarks}
We rarely check for this assumption

\subsubsection{Unusual Observations}
\paragraph{Definition 5.2.2.1 - Leverage points}
A \under{leverage point} is a point whose $x$-value is distant from the other $x$-values
\paragraph{leverages}
In the linear regression model, the \under{leverage} for the $i$-th observation is defines as:
$$h_i = H_{ii} = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}$$
where $H = X(X^TX)^{-1}X^T$ \\
Property: $\sum_{i=1}^n h_i = 2$ (number of parameters)
\paragraph{Remarks}
The average value for $h$ is $2/n$. Usually leverages larger than $4/n$ should be looked at more closely.
\paragraph{Definition 5.2.2.2 - Outliers}
An \under{outlier} is a data point whose $y$-value differs significantly from other observations.
\paragraph{Remarks}
Usually large residual $\hat{y}_i - y_i$ might indicate outliers
\paragraph{Definition 5.2.2.3 - Influential observations}
An \under{influential point} is one whose removal from the dataset would cause a large change in the fit. They could be leverage points, outliers but usually the both.
\paragraph{Remarks}
An outlier with a large leverage will definitely be an influential observation. It is sometimes called a \under{bad leverage}
\paragraph{Definition 5.2.2.4 - Cook's Distance}
For observation $i$, the \under{Cook's distance} is $$D_i = \frac{r_i^2}{2} \frac{h_i}{1-h_i}$$ where $r_i$ is the standardized residual and $h_i$ is the leverage.
\paragraph{Remarks}
$r_i$ measures the extent of outlying, $h_i$ measures the leverage. Thus, a large value of $D_i$ indicates influential observations.
\paragraph{Simple rules of thumb} There is a problem when
\begin{enumerate}
    \item $D_i > 4/n$ on large datasets
    \item $D_i > 1$ on small datasets
    \item $D_i$ is separated by a large gap from the other $D_j$s
\end{enumerate}

\section{May 28th - Dummy variables and introduction to multiple linear regression}
\subsection{Transformation}
We can use transformations to fix 2 problems:
\begin{enumerate}
	\item Non-constant variance
	\item Non-linearity
\end{enumerate}
When the \textcolor{blue}{variance is exploding}, typically we raise $y$ to a power between 0 and 1 or apply a logarithmic transformation.
\paragraph{Logarithmic Transformation}
$$\log(y_i) = \beta_0 + \beta_1 x_i + e_i$$
$$y_i = \exp(\beta_0)\exp(\beta_1x_i)\exp(e_i)$$
\paragraph{Box-Cox Transformation}
Let's consider a family of possible transformations $g_\lambda(y)$
$$g_\lambda(y)=\begin{cases}
	\frac{y^\lambda-1}{\lambda} & \text{if $\lambda \neq 0$} \\
	\log(y) & \text{if $\lambda = 0$}
\end{cases}$$
$\lambda$ is selected by the model achieving the highest log-likelihood:
$$g_\lambda(y_i) \sim N(\beta_0 + \beta_1x_i, \sigma^2)$$
$$l(\lambda|\vy) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n(g_\lambda(y_i) - \beta_0 - \beta_1x_i)^2$$
\paragraph{Conclusions}
\begin{enumerate}
	\item Transforming the response (or the predictors) might help with violated assumptions
	\item It makes the model less interpretable.
\end{enumerate}
\subsection{Multiple Linear Regression}
Suppose we have $p$ predictors.
$$y = \beta_0 + \beta_1x_1 + \hdots + \beta_p x_p + e$$ where $e \sim N(0, \sigma^2)$
$$y = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix},
X = \begin{bmatrix}
	1 & x_{11} & \hdots & x_{1p} \\
	\vdots & \vdots & \hdots & \vdots \\
	1 & x_{n1} & \hdots & x_{np}
\end{bmatrix}, \beta = \begin{bmatrix}
	\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{bmatrix}$$
Matrix notation:
$$\vy = X\beta + \ve$$ where $\ve \sim N(0, I\sigma^2)$ (a vector of independent normal variables)
Therefore,
$$ \vy \sim N(X\beta, I\sigma^2)$$
\paragraph{Inference}
$$l(\theta|x_i) = -\frac{n}{2} \log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}(\vy - X\beta)^T(\vy - X\beta)$$
which leads to $$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
\subsection{Dummy variables}
A set of binary variables to represent a categorical variable.
Allows to fit a parameter for every possible categories of a categorical variable.
\paragraph{Model - 2 groups}
Fit a linear regression:
$$y = \beta_0 + \beta_1x + e$$ where $e \sim N(0, \sigma^2)$
$$y \sim N(\beta_0 + \beta_1 x, \sigma^2)$$
This is a model that consists only \textcolor{red}{two different intercepts}, no slope.
\paragraph{Inference}
$$E(y) = \begin{cases}
	\beta_0 & \text{if $x = 0$} \\
	\beta_0 + \beta_1 &\text{if $x = 1$}
\end{cases}$$
$$H_0: \beta_1 = 0$$
Applying t-test, this is exactly same to lecture 2.
\paragraph{Model - 3 groups}
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + e$$
where $e \sim N(0, \sigma^2)$
\paragraph{Inference}
$$E(y) = \begin{cases}
	\beta_0 & \text{for Group A} \\
	\beta_0 + \beta_1 &\text{for Group B} \\
	\beta_0 + \beta_2 &\text{for Group C}
\end{cases}$$
$H_0: \beta_1 = \beta_2 = 0$ for testing if all groups are the same; $H_0: \beta_1 = 0$ for testing if Group B is same as Group A.

\section{May 30th - Interactions and multiple linear regression assumptions}
\subsection{Interactions}
\paragraph{Definition} \under{Interaction} is the effect of predictor $x_1$ on the effect of predictor $x_2$ on $y$. With the interaction term, the linear regression model becomes
$$y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \beta_{1,2}x_1x_2 + e$$
where $e \sim N(0,1)$
\paragraph{Remarks}
As $x_1$ varies, 
\begin{enumerate}
	\item the effect of $x_2$ on $y$ is different.
	\item the relationship between $x_2$ and $y$ is different.
\end{enumerate}
Interaction between two categorical predictors $\implies$ every combination are categories with respective effects\\
Interaction between a categorical predictor and a numerical predictor $\implies$ different intercepts and different slopes\\
\paragraph{Example: numerical and categorical}
Suppose $x_1$ is numerical, $x_2$ is categorical.\\
Then for a fixed $x_1$, 
$$E(y) = \begin{cases}
	\beta_0 + \beta_1x_1 & \text{if $x_2 = 0$} \\
	(\beta_0 + \beta_2) + (\beta_1 + \beta_{1,2}x_2) & \text{if $x_2 = 1$}
\end{cases}$$
Now $\beta_2$ indicates difference in intercept and $\beta_{1,2}$ indicates difference in slope (interaction)
\paragraph{Example: categorical and categorical}
$\beta_{1,2}$ allow for a different effect from change of predictor 1 depending on the value of predictor 2.
\paragraph{Example: numerical and numerical}
More complicated. \\
The slope and intercept of $x_1$ is different across different values of $x_2$.
\paragraph{Remarks}
The model is completely wrong without the interaction term.But interaction terms make the number of parameters explode quickly. ($p$ predictors $\implies {p \choose 2}$ interaction terms)

\subsection{Polynomial fit}
\begin{enumerate}
	\item Can be understood as a special case of interactions. 
	\item Can also solve the issue of observable pattern in the residuals
\end{enumerate}
$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

\subsection{Model Checking}
\subsubsection{Collinearity}
\paragraph{Correlation}
$$\rho_{X,Y} = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}$$
range: $[-1, 1]$.\\
when $\rho = 1/-1$, we call it "perfect correlation".
\paragraph{Theorem}
$\rho = \pm 1$ iff $P(Y = a + bX) = 1$ for some constants $a$ and $b$. \\
\proof \\
Rice p143.
\paragraph{Collinearity}
If two variables are perfectly correlated, it implies a perfect increasing(decreasing) linear relationship ($x_i = a+ bx_j$). This implies that $\det(X) = 0 = \det(X^TX) = 0$.\\
$\implies X^TX$ not invertible \\
$\implies \hat{\beta} = (X^TX)^{-1}X^T\vy$ does not exist \\
$\implies$ perfect correlation should not happen. \\
We also do not want a correlation close to -1 or 1, since the variance of $\hat{\beta}$ would be extremely large.
\paragraph{Checking for collinearity}
\begin{enumerate}
	\item Look at the correlation matrix of the predictors. Large pairwise correlation indicates a problem.
	\item Build a regression model for $x_i$ as response on all other predictors to assess $R_i^2$. High $R_i^2$ (close to 1) indicates a problem.
	\item Look at the eigenvalues of $X^TX$. Small eigenvalues indicates a problem.
\end{enumerate}

\paragraph{Variance Inflating Factor (VIF)}
$$Var(\hat{\beta_j}) = \frac{1}{1-R_j^2} \times \frac{\sigma^2}{(n-1)SSX_j}$$
$\frac{1}{1-R_j^2}$ are defined as the VIF.

\paragraph{Remarks}
If the variable $X_j$ is uncorrelated then VIF = 1





\section{June 6th - Model selection and variable selection}
\section{June 11th - Ridge and Lasso regression}
\section{June 13th - Statistical analysis, data science and ethics}

\end{document}
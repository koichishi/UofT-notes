\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by Yuchen Wang}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof: }}}
\newcommand{\stdn}[0]{N(0, 1)}
\newcommand{\qed}[0]{$\hfill\blacksquare$}


% Attr.
\title{STA261 Probability and Statistics II \\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
\section{Normal Distribution Theory}
\paragraph{Theorem: Sum of independent normal random variables}
Suppose $X_i \sim N(\mu_i, \sigma^2_i)$ for $i = 1, 2,...,n$ and that they are independent random variables. Let $Y = (\Sigma_ia_iX_i) + b$ for some constants $\{a_i\}$ and $b$. Then
$$Y \sim N((\Sigma_ia_i\mu_i)+b, \Sigma_ia^2_i\sigma^2_i)$$
\paragraph{Corollary: The distribution of the sample mean of normal random variables} Suppose $X_i \sim N(\mu, \sigma^2)$ for $i=1,2,...,n$ and that they are independent random variables, If $\bar X = (X_1 + ... +X_n)/n$, then $\bar X \sim N(\mu,\sigma^2/n)$
\paragraph{Theorem: The covariance of sums of normal random variables}Suppose $X_i \sim N(\mu_i, \sigma_i^2)$ for $i=1,2,...,n$ and also that the $\{X_i\} $are independent. Let $U=\Sigma^n_{i=1}a_iX_i$ and $V=\Sigma^n_{i=1}b_iX_i$ for some constants $\{a_1\}$ and $\{b_i\}$. Then $Cov(U,V) = \Sigma_ia_ib_i\sigma^2$. Furthermore, $Cov(U,V) = 0$ if and only if U and V are independent.

\section{Expectation and Covariance}
\subsection{Expectation -Discrete case}
\paragraph{Definition of expectation} Let X be a discrete random variable, taking on distince values $x_1,x_2,...$, with $p_i = P(X = x_i)$. Then the \ti{expected value} (or \ti{mean} or \ti{mean value}) of X, written E(X) (or $\mu_x$), is defined by $$E(X) = \Sigma_i x_ip_i$$
\paragraph{Theorem: expectation involving nested functions} 
\begin{enumerate}
\item Let X be a discrete random variable, and let $g: \mb{R} \rightarrow \mb{R}$ be some function such that the expectation of the random variable $g(X)$ exists. Then $$E(g(X)) = \Sigma_x g(x)P(X=x)$$
\item Let X and Y be discrete random variables, and let $h: \mb{R}^2 \rightarrow \mb{R}$ be some function such that the expectation of the random variable $h(X,Y)$ exists. Then
$$E(h(X,Y)) = \Sigma_{x,y}h(x,y)P(X=x,Y=y)$$
\end{enumerate}
\paragraph{Theorem: Linearity of expected values} Let X and Y be discrete random variables, let $a$ and $b$ be real numbers, and put $Z = aX + bY$. Then $$E(Z) = aE(X) + bE(Y)$$
\paragraph{Theorem: Expectation of product of independent r.v}Let X and Y be discrete random variables that are independent. Then $$E(XY) = E(X)E(Y)$$
\paragraph{Monotonicity} Let X and Y be discrete random variables, and suppose that $X \leq Y$ (Remember that this means $X(s) \leq Y(s)$ for all $s\in S)$ Then $E(X) \leq E(Y)$.
\subsection{Expectation - Continuous case}
\paragraph{Definition of expectation} Let X be an absolutely continuous random variable, with density function $f_X$. Then the \ti{expected value} of X is given by
$$ E(x) = \int_{-\infty}^{\infty}xf_X(x)dx$$
\paragraph{Theorem: expectation involving nested functions} 
\begin{enumerate}
\item Let X be a an absolutely continuous random variable with density function $f_X$, and let $g: \mb{R} \rightarrow \mb{R}$ be some function such that the expectation of the random variable $g(X)$ exists. Then $$\int_{-\infty}^{\infty} =  g(x)f_X(x)dx$$
\item Let X and Y be discrete random variables, and let $h: \mb{R}^2 \rightarrow \mb{R}$ be some function such that the expectation of the random variable $h(X,Y)$ exists. Then
 $$E(h(X,Y)) = \int_{-\infty}^{\infty}h(x,y)f_{X,Y}(x,y)dxdy$$
 \end{enumerate}
 \paragraph{Theorem: Linearity of expected values} Let X and Y be jointly absolutely continuous random variables, let $a$ and $b$ be real numbers. Then $$E(aX + bY) = aE(X) + bE(Y)$$
\paragraph{Monotonicity} Let X and Y be jointly continuous random variables, and suppose that $X \leq Y$ (Remember that this means $X(s) \leq Y(s)$ for all $s\in S)$ Then $E(X) \leq E(Y)$.
\subsection{Variance, Covariance and Correlation}
\paragraph{Definition of variance} The \ti{variance} of a random variable X is the quantity
$$\sigma^2_x = Var(X) = E((X - \mu_X)^2)$$
where $\sigma_X$ is the \ti{standard deviation} of X.
\paragraph{Theorem} Let X be any r.v. with $\mu_X = E(X)$ and variance Var(X). Then the following hold true:
\begin{enumerate}
	\item $Var(X) \geq 0$
	\item If $a$ and $b$ are real numbers, $Var(aX+b) = a^2Var(X)$
	\item $Var(X) = E(X^2) - (\mu_X)^2 = E(X^2) - E(X)^2$
	\item $Var(X) \leq E(X^2)$
\end{enumerate}
\paragraph{Definition of covariance} $$Cov(X,Y) = E((X - \mu_X)(Y - \mu_Y))$$
\paragraph{Theorem: Linearity of covariance} Let X, Y and z be three r.v.s. Let $a$ and $b$ be real numbers. Then
$$Cov(aX + bY. Z) = aCov(X,Z) + bCov(Y,Z)$$
\paragraph{Theorem} Let X and Y be r.v.s. Then
$$Cov(X,Y) = E(XY) - E(X)E(Y)$$
\paragraph{Theorem} If X and Y are independent, then $$Cov(X,Y) = 0$$.
\paragraph{Theorem} 
\begin{enumerate}
\item For any r.v.s X and Y,
$$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$
\item More generally, for any r.v.s $X_1,...,X_n$,$$Var(\Sigma_i X_i) = \Sigma_i Var(X_i) + 2\Sigma_{i < j}Cov(X_i,X_j)$$
\end{enumerate}
\paragraph{Corollary}
\begin{enumerate}
	\item If X and Y are independent, then $Var(X+Y) = Var(X) + Var(Y)$
	\item If $X_1,...X_n$ are independent, then $Var(\Sigma_{i=1}^n X_i = \Sigma_{i=1}^nVar(X_i)$
\end{enumerate}
\paragraph{Definition} The \ti{correlation} of two r.v.s X and Y is given by
$$Corr(X,Y) = \frac{Cov(X,Y)}{Sd(X)Sd(Y)}$$ 
provided $Var(X) < \infty$ and $Var(Y) < \infty$
\section{Types of Inferences}
\paragraph{Estimation:}
\begin{enumerate}
	\item Point estimation: Based on the sample observations, calculating a particular value as an estimate of the parameter $\theta$
	\item Interval estimation: Calculating a range of values that is likely to contain the parameter $\theta$
\end{enumerate}
\paragraph{Hypothesis testing} Based on the sample, assess whether a hypothetical value $\theta_0$ is a plausible value of the parameter $\theta$ or not.
\section{Different Types of Estimation}
\subsection{Method of Moments Estimation}
Let $X_1, X_2, ..., X_n$ are independently and identically distributed (i.i.d.) random variables. \newline
Let the $k^{th}$ population moment be
$$\mu_k = E[X^k]$$
$k^{th}$ sample moment based on sample
$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}X^k_i$$
We use $\hat{\mu}_k$ as an estimator of $\mu_k$ \newline
In other words, we use the sample moments as estimators of the population moments.
\subsection{Maximum Likelihood Estimation}
\paragraph{Definition of Likelihood Function}
Suppose $X_1, X_2,...,X_n$ has a joint density or mass function $f(x_1, x_2,...,x_n|\theta)$ \newline
We observe sample, $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$ \newline
Given the sample, the likelihood function of $\theta$, noted as $L(\theta|x_1, x_2,...,x_n)$, is defined as
$$L(\theta|x_1, x_2,...,x_n) = f(x_1, x_2,...,x_n|\theta)$$
Often written as $L(\theta)$, is a function of $\theta$. \newline
If X follows a discrete distribution, it gives the probability of observing the sample as a function of the parameter $\theta$ \newline
If $X_1, X_2,...,X_n$ are i.i.d. then their joint density is the product of marginal densities, $f_\theta(x)$\newline
Hence, in i.i.d. case we write
$$L(\theta) = \Pi_{i=1}^n f_\theta(x_i)$$
\paragraph{Comments}
\begin{enumerate}
	\item $L(\theta)$ is NOT a pdf or pmf of $\theta$
	\item Likelihood introduces a belief ordering on parameter space, $\Omega$
	\item For $\theta_1, \theta_2 \in \Omega$, we believe in $\theta_1$ as the true value of $\theta$ over $\theta_2$ whenever $L(\theta_1) > L(\theta_2)$
	\item Which means, the data is more likely to come from $f_{\theta_1}$ than $f_{\theta_2}$
	\item The value $L(\theta)$ is very small for every value of $\theta$
	\item So often, we are interested in the likelihood ratios:
	$$\frac{L(\theta_1)}{L(\theta_2)}$$
\end{enumerate}
\paragraph{Maximum Likelihood Estimation}
\begin{enumerate}
	\item Let's say we are interested in a point estimate of $\theta$
	\item A sensible choice will be to pick $\hat{\theta}$ that maximizes $L(\theta)$
	\item So $\hat{\theta} satisfies L(\hat{\theta} \geq L(\theta)$ for all $\theta \in \Omega$
	\item $\hat{\theta}$ is called the \under{maximum likelihood estimate} (MLE) of $\theta$
\end{enumerate}
\paragraph{Computation of the MLE}
\begin{enumerate}
	\item Define, log-likelihood function, $l(\theta) = \ln{L(\theta)}$
	\item $\ln(x)$ is a 1-1 increasing function of $x > 0 \implies L(\hat{\theta}) \geq L(\theta)$ for $\theta \in \Omega$ iff $l(\hat{\theta}) \geq l(\theta)$
	\item In other words, if $L(\theta)$ is maximized at $\hat{\theta}$ then $l(\theta)$ will also be maximized at $\hat{\theta}$
	\item Therefore, $$l(\theta) = \ln{(\Pi_{i=1}^nf_\theta(x_i))} = \sum_{i=1}^n\ln{f_\theta(x_i)}$$
	\item The obvious benefit: It's much easier to differentiate a sum than a product
	\item Solve the equation, $\frac{\partial l(\theta)}{\partial \theta} = 0$ for $\theta$
	\item Say, $\hat{\theta}$ is the solution. But it's still not the MLE
	\item Need to check whether or not $$\frac{\partial^2 l(\theta)}{\partial \theta^2}\vert_{\theta = \hat{\theta}} < 0$$
\end{enumerate}
\paragraph{Properties of MLE}
\begin{enumerate}
	\item MLE is not unique
	\item MLE may not exists
	\item The likelihood may not always be differentiable.
\end{enumerate}
\section{Sampling Distribution of an Estimator}
\begin{enumerate}
	\item Recall: An Estimator (T) is a random variable (infinite number of sample means)
	\item If we repeat the sampling procedure and keep calculating T for each set of sample and finally draw a density histogram based on the T values we get the sampling distribution of T
	\item \tb{Standard error:} Standard deviation of an estimator is called the standard error (SE)
\end{enumerate}

\paragraph{Definition of Mean Squared Error}
Let $\psi(\theta)$ be any real valued function of $\theta$, suppose T is an estimator of $\psi(\theta)$
$$MSE_\theta(T) = E_\theta[(T - \psi(\theta))^2]$$
\paragraph{Corollary}
$$MSE_\theta(T) = Var_\theta(T) + (E_\theta(T) - \psi(\theta))^2$$
\proof
\begin{align*}
	MST(T) &= E[(T - \psi({\theta}))^2] \\
	&= E[(T - E(T) + E(T) - \psi({\theta}))^2] \\
	&= E[(T - E(T))^2 + (E(T) - \psi({\theta}))^2 + 2(T-E(T))(E(T)-\psi({\theta}))] \\
	&= E[(T - E(T))^2] + (E(T) - \psi({\theta}))^2 +  2E[T-E(T)](E(T)-\psi({\theta}))\\
	&= E[(T - E(T))^2] + (E(T) - \psi({\theta}))^2 \tag{Since $E[T-E(T)] =  E(T)-E(T) = 0$}\\
	&= Var(T) + (E(T) - \psi({\theta}))^2\\
	&= Var(T) + Bias^2(T)
\end{align*}\qed

\paragraph{Bias} The bias of an estimator T of $\psi(\theta)$ is given by $$E_\theta(T) - \psi(\theta)$$
\paragraph{Unbiased estimator:} When the bias of an estimator is zero, it's called unbiased
\paragraph{Remark}
\begin{enumerate}
	\item For unbiased estimators, $$MSE_\theta(T) = Var_\theta(T)$$
	\item If all the other properties are similar, then an unbiased estimator is preferred over a biased estimator.
	\item In practice, often an biased estimator with lower variance is preferred over an unbiased estimator with really high variance. \tb{We minimize MSE}.
\end{enumerate}

\end{document}